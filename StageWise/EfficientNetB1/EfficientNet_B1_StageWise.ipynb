{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df67a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['stage1', 'stage2', 'stage3', 'stage4', 'stage5', 'stage6', 'stage7', 'stage8', 'stage9']\n"
     ]
    }
   ],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "#dataloader\n",
    "train_path =  r\"C:\\Datasets\\Train_set\"\n",
    "test_path = r\"C:\\Datasets\\Test_set\"\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=28, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=14, shuffle=True\n",
    ")\n",
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d960a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import ceil\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU() # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4, # squeeze excitation\n",
    "            survival_prob=0.8, # for stochastic depth\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                in_channels, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
    "            ),\n",
    "            SqueezeExcitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            return self.stochastic_depth(self.conv(x)) + inputs\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate\n",
    "\n",
    "    def create_features(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNNBlock(1, channels, 3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4*ceil(int(channels*width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size//2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e8fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77626 23645\n"
     ]
    }
   ],
   "source": [
    "model=EfficientNet(\"b1\", num_classes=23).to(device)\n",
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cf40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2ac722",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "best_accuracy= 0.0\n",
    "test_accuracy=0.0\n",
    "train_accuracy=0.0\n",
    "train_loss=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04e8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(start_epochs, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred):\n",
    "    #Model training and saving best model\n",
    "\n",
    "    for epoch in range(start_epochs, num_epochs+1):\n",
    "\n",
    "        #Evaluation and training on training dataset\n",
    "        model.train()\n",
    "\n",
    "        for i, (images,labels) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images=Variable(images.cuda())\n",
    "                labels=Variable(labels.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs=model(images)\n",
    "            loss=loss_function(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss+= loss.cpu().data*images.size(0)\n",
    "            _,prediction=torch.max(outputs.data,1)\n",
    "\n",
    "            train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "\n",
    "        train_accuracy=train_accuracy/train_count\n",
    "        train_loss=train_loss/train_count\n",
    "\n",
    "        # Evaluation on testing dataset\n",
    "        model.eval()\n",
    "\n",
    "        for i, (images,labels) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images=Variable(images.cuda())\n",
    "                labels=Variable(labels.cuda())\n",
    "\n",
    "            outputs=model(images)\n",
    "            _,prediction=torch.max(outputs.data,1)\n",
    "            test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "\n",
    "            output = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "            y_pred.extend(output) # Save Prediction\n",
    "\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels) # Save Truth\n",
    "\n",
    "        test_accuracy=test_accuracy/test_count\n",
    "\n",
    "        print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        checkpoint = {\n",
    "                'start_epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'train_loss': train_loss,\n",
    "                'train_losses':train_losses,\n",
    "                'test_accuracies': test_accuracies,\n",
    "                'y_true': y_true,\n",
    "                'y_pred': y_pred\n",
    "        }\n",
    "        save_ckp(checkpoint, False, checkpoint_path='current_checkpoint_efficientNet_b1.pt', best_model_path='best_checkpoint_efficientNet_b1_stagewise.model')\n",
    "        #Save the best model\n",
    "        if test_accuracy>best_accuracy:\n",
    "            torch.save(model.state_dict(),'best_checkpoint_efficientNet_b1_stagewise.model')\n",
    "            best_accuracy=test_accuracy\n",
    "            checkpoint['best_accuracy']=best_accuracy\n",
    "            save_ckp(checkpoint, True, checkpoint_path='current_checkpoint_efficientNet_b1_stagewise.pt', best_model_path='best_checkpoint_efficientNet_b1_stagewise.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09df25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    train_accuracy = checkpoint['train_accuracy']\n",
    "    test_accuracy = checkpoint['test_accuracy']\n",
    "    train_loss = checkpoint['train_loss']    \n",
    "    train_losses = checkpoint['train_losses']    \n",
    "    test_accuracies = checkpoint['test_accuracies']   \n",
    "    start_epoch = checkpoint['start_epoch']\n",
    "    y_true = checkpoint['y_true']\n",
    "    y_pred = checkpoint['y_pred']\n",
    "    return model, optimizer,start_epoch,best_accuracy, train_accuracy,test_accuracy,train_loss.item(),train_losses,test_accuracies,y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbee9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y_true,y_pred):\n",
    "     # constant for classes\n",
    "    #classes = ('L4', 'day1', 'day10', 'day11', 'day12', 'day13', 'day14', 'day15', 'day16', 'day17', 'day18', 'day19', 'day2', 'day20', 'day21', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9', 'others')\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix), index = [i for i in classes],\n",
    "                         columns = [i for i in classes])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fdec22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_curve_train_losses(train_losses):\n",
    "    plt.plot(train_losses,'-o')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('losses')\n",
    "    plt.legend(['Train'])\n",
    "    plt.title('Train Losses Per Epoch')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def learn_curve_test_accuracies(test_accuracies):\n",
    "    plt.plot(test_accuracies,'-r')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('test_accuracy')\n",
    "    plt.legend(['Test Accuracy'])\n",
    "    plt.title('Test Accuracy Per Epoch')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a29afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: tensor(1.6431) Train Accuracy: 0.3267977224125937 Test Accuracy: 0.4441530979065342\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3342df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667188cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: tensor(1.0753) Train Accuracy: 0.570433339273439 Test Accuracy: 0.5384429658104101\n",
      "Epoch: 4 Train Loss: tensor(0.8725) Train Accuracy: 0.6620922169548769 Test Accuracy: 0.6677326471966932\n",
      "Epoch: 5 Train Loss: tensor(0.7408) Train Accuracy: 0.7161989809112533 Test Accuracy: 0.6888842348338844\n",
      "Epoch: 6 Train Loss: tensor(0.6526) Train Accuracy: 0.7536742354234524 Test Accuracy: 0.7138798428519701\n",
      "Epoch: 7 Train Loss: tensor(0.5828) Train Accuracy: 0.7789626371864508 Test Accuracy: 0.716333849855904\n",
      "Epoch: 8 Train Loss: tensor(0.5284) Train Accuracy: 0.8016744256130316 Test Accuracy: 0.7386219637914933\n",
      "Epoch: 9 Train Loss: tensor(0.4825) Train Accuracy: 0.8204957317706132 Test Accuracy: 0.773133373734988\n",
      "Epoch: 10 Train Loss: tensor(0.4426) Train Accuracy: 0.8355940083957923 Test Accuracy: 0.7800284683177727\n",
      "Epoch: 11 Train Loss: tensor(0.4163) Train Accuracy: 0.8465312600676114 Test Accuracy: 0.7786331160274188\n",
      "Epoch: 12 Train Loss: tensor(0.3851) Train Accuracy: 0.8577261037701294 Test Accuracy: 0.7789291026904642\n",
      "Epoch: 13 Train Loss: tensor(0.3631) Train Accuracy: 0.8671174313516575 Test Accuracy: 0.7942811981011922\n",
      "Epoch: 14 Train Loss: tensor(0.3371) Train Accuracy: 0.8760320912765227 Test Accuracy: 0.7983419023555974\n",
      "Epoch: 15 Train Loss: tensor(0.3246) Train Accuracy: 0.8832849307202647 Test Accuracy: 0.8093803485685073\n",
      "Epoch: 16 Train Loss: tensor(0.3048) Train Accuracy: 0.8895200485008982 Test Accuracy: 0.8089578930153761\n",
      "Epoch: 17 Train Loss: tensor(0.2919) Train Accuracy: 0.8951496859305967 Test Accuracy: 0.805701372716981\n",
      "Epoch: 18 Train Loss: tensor(0.2696) Train Accuracy: 0.9028404806338848 Test Accuracy: 0.7895878917899224\n",
      "Epoch: 19 Train Loss: tensor(0.2631) Train Accuracy: 0.9061126792631417 Test Accuracy: 0.8017250830150895\n",
      "Epoch: 20 Train Loss: tensor(0.2495) Train Accuracy: 0.9108018719588703 Test Accuracy: 0.8169508024987531\n",
      "Epoch: 21 Train Loss: tensor(0.2450) Train Accuracy: 0.9131980367643825 Test Accuracy: 0.8222379763502854\n",
      "Epoch: 22 Train Loss: tensor(0.2338) Train Accuracy: 0.9166891659757912 Test Accuracy: 0.8318385382946225\n",
      "Epoch: 23 Train Loss: tensor(0.2239) Train Accuracy: 0.9209790107588435 Test Accuracy: 0.808451335950023\n",
      "Epoch: 24 Train Loss: tensor(0.2197) Train Accuracy: 0.9221900005025475 Test Accuracy: 0.808915561485978\n",
      "Epoch: 25 Train Loss: tensor(0.2104) Train Accuracy: 0.925835701826714 Test Accuracy: 0.8383086874840976\n",
      "Epoch: 26 Train Loss: tensor(0.2073) Train Accuracy: 0.9266473325393789 Test Accuracy: 0.8192361306275103\n",
      "Epoch: 27 Train Loss: tensor(0.2011) Train Accuracy: 0.9286569789417533 Test Accuracy: 0.8251562375187409\n",
      "Epoch: 28 Train Loss: tensor(0.1919) Train Accuracy: 0.9324186310898274 Test Accuracy: 0.8405508630254819\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae401d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Train Loss: tensor(0.1870) Train Accuracy: 0.9345829028757259 Test Accuracy: 0.8418202812798911\n",
      "Epoch: 30 Train Loss: tensor(0.1842) Train Accuracy: 0.9347246358552918 Test Accuracy: 0.8423701340782948\n",
      "Epoch: 31 Train Loss: tensor(0.1808) Train Accuracy: 0.9359613367252706 Test Accuracy: 0.8395788695341121\n",
      "Epoch: 32 Train Loss: tensor(0.1778) Train Accuracy: 0.9371594048558051 Test Accuracy: 0.8490945053444506\n",
      "Epoch: 33 Train Loss: tensor(0.1720) Train Accuracy: 0.9393107613351822 Test Accuracy: 0.8423704417215202\n",
      "Epoch: 34 Train Loss: tensor(0.1680) Train Accuracy: 0.9405346058119876 Test Accuracy: 0.8452037373838749\n",
      "Epoch: 35 Train Loss: tensor(0.1642) Train Accuracy: 0.9423896701440989 Test Accuracy: 0.8304861579081152\n",
      "Epoch: 36 Train Loss: tensor(0.1621) Train Accuracy: 0.9429178675916593 Test Accuracy: 0.8487980751176954\n",
      "Epoch: 37 Train Loss: tensor(0.1607) Train Accuracy: 0.943974221496246 Test Accuracy: 0.842328136945448\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 45\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c0bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Train Loss: tensor(0.1691) Train Accuracy: 0.9401867564914443 Test Accuracy: 0.8471917570101647\n",
      "Epoch: 34 Train Loss: tensor(0.1679) Train Accuracy: 0.9412946717176782 Test Accuracy: 0.8287945524109541\n",
      "Epoch: 35 Train Loss: tensor(0.1693) Train Accuracy: 0.9403413971436339 Test Accuracy: 0.8343340577099772\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 45\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c07c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Train Loss: tensor(0.1730) Train Accuracy: 0.9393880550254406 Test Accuracy: 0.8493063689788685\n",
      "Epoch: 34 Train Loss: tensor(0.1695) Train Accuracy: 0.9402640788918022 Test Accuracy: 0.8356036923818557\n",
      "Epoch: 35 Train Loss: tensor(0.1664) Train Accuracy: 0.9417713171370274 Test Accuracy: 0.8449919900060215\n",
      "Epoch: 36 Train Loss: tensor(0.1595) Train Accuracy: 0.9427375076819252 Test Accuracy: 0.8435544509194335\n",
      "Epoch: 37 Train Loss: tensor(0.1594) Train Accuracy: 0.9440257482996377 Test Accuracy: 0.8406362256058753\n",
      "Epoch: 38 Train Loss: tensor(0.1583) Train Accuracy: 0.9441674699939233 Test Accuracy: 0.8406783944269658\n",
      "Epoch: 39 Train Loss: tensor(0.1532) Train Accuracy: 0.9458679330053074 Test Accuracy: 0.8535352369800984\n",
      "Epoch: 40 Train Loss: tensor(0.1526) Train Accuracy: 0.9462157765173139 Test Accuracy: 0.8557349771722131\n",
      "Epoch: 41 Train Loss: tensor(0.1521) Train Accuracy: 0.9466408962947533 Test Accuracy: 0.813950337702566\n",
      "Epoch: 42 Train Loss: tensor(0.1475) Train Accuracy: 0.9487664782533725 Test Accuracy: 0.8407618502997548\n",
      "Epoch: 43 Train Loss: tensor(0.1452) Train Accuracy: 0.9485990359734915 Test Accuracy: 0.8560304826327046\n",
      "Epoch: 44 Train Loss: tensor(0.1426) Train Accuracy: 0.9495136758178441 Test Accuracy: 0.849222077838132\n",
      "Epoch: 45 Train Loss: tensor(0.1452) Train Accuracy: 0.9500160965871721 Test Accuracy: 0.8262571039153241\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 45\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b534b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 Train Loss: tensor(0.1430) Train Accuracy: 0.9500418493679433 Test Accuracy: 0.8669002338964953\n",
      "Epoch: 45 Train Loss: tensor(0.1464) Train Accuracy: 0.9492818133337976 Test Accuracy: 0.8642362825220511\n",
      "Epoch: 46 Train Loss: tensor(0.1390) Train Accuracy: 0.9515104382785836 Test Accuracy: 0.859795484723304\n",
      "Epoch: 47 Train Loss: tensor(0.1374) Train Accuracy: 0.9516779366505848 Test Accuracy: 0.8483763922810201\n",
      "Epoch: 48 Train Loss: tensor(0.1353) Train Accuracy: 0.9529275201341902 Test Accuracy: 0.857257279610585\n",
      "Epoch: 49 Train Loss: tensor(0.1366) Train Accuracy: 0.952811595696289 Test Accuracy: 0.8568770250488311\n",
      "Epoch: 50 Train Loss: tensor(0.1331) Train Accuracy: 0.9530305929919832 Test Accuracy: 0.8504062963427806\n",
      "Epoch: 51 Train Loss: tensor(0.1334) Train Accuracy: 0.953378417419331 Test Accuracy: 0.8554387991666883\n",
      "Epoch: 52 Train Loss: tensor(0.1284) Train Accuracy: 0.9546795323527866 Test Accuracy: 0.8519710483738282\n",
      "Epoch: 53 Train Loss: tensor(0.1286) Train Accuracy: 0.9553751923264416 Test Accuracy: 0.8614866555740484\n",
      "Epoch: 54 Train Loss: tensor(0.1257) Train Accuracy: 0.9564830775151666 Test Accuracy: 0.8573424185517264\n",
      "Epoch: 55 Train Loss: tensor(0.1253) Train Accuracy: 0.9561867993079318 Test Accuracy: 0.8636014947100255\n",
      "Epoch: 56 Train Loss: tensor(0.1274) Train Accuracy: 0.9554782699971569 Test Accuracy: 0.8573425079930096\n",
      "Epoch: 57 Train Loss: tensor(0.1222) Train Accuracy: 0.957616719633499 Test Accuracy: 0.8458387541766966\n",
      "Epoch: 58 Train Loss: tensor(0.1204) Train Accuracy: 0.9584927423378717 Test Accuracy: 0.8688029536373092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 58\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"current_checkpoint_efficientNet_b1_stagewise.pt\"):\n",
    "    ckp_path = \"current_checkpoint_efficientNet_b1_stagewise.pt\"\n",
    "    model, optimizer, start_epoch, best_accuracy, train_accuracy,test_accuracy,train_loss,train_losses,test_accuracies,y_true,y_pred  = load_ckp(ckp_path, model, optimizer)\n",
    "    train_test(start_epoch, best_accuracy,train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)\n",
    "else:\n",
    "    train_test(1,best_accuracy, train_accuracy,test_accuracy,train_loss, train_losses, test_accuracies,y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b0427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
